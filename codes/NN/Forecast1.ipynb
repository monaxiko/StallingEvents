{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% IMPORT SECTION\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "import os\n",
    "import tempfile \n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import TimeDistributed, RepeatVector,LSTM,Flatten,Dense,Conv1D,CuDNNLSTM,CuDNNGRU\n",
    "from keras.regularizers import l1_l2, l1, l2\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "colors = np.array(['#0101DF','#DF0101','#01DF01','#DFDFDF'])\n",
    "\n",
    "rs = 42\n",
    "def compute_metrics (y_te2,y_p,string):\n",
    "    print('\\n %s'%string)\n",
    "    print('CONFUSION MATRIX')\n",
    "    print(metrics.confusion_matrix(y_te2,y_p))\n",
    "    print(classification_report(y_te2, y_p))\n",
    "    print('\\nROC CURVE: %2.2f'%roc_auc_score(y_te2,y_p))\n",
    "\n",
    "def Ratio10(y,string):\n",
    "    suma = Counter(y)[0]+Counter(y)[1]\n",
    "    print('\\n'+string)  \n",
    "    print(Counter(y))\n",
    "    print('0: %3.2f%%\\n1: %3.2f%%' % (100*Counter(y)[0]/suma,100*Counter(y)[1]/suma))\n",
    "\n",
    "def histo(data,var):\n",
    "    fig=plt.figure()\n",
    "    sns.countplot(var,data=data,palette=colors)\n",
    "    plt.title('0/1 Distribution (0: No Stalling || 1: Stalling Event',fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_loss(history, label, n):\n",
    "    # Use a log scale to show the wide range of values.\n",
    "    plt.semilogy(history.epoch,  \n",
    "                 history.history['loss'],\n",
    "                 color=colors[n], \n",
    "                 label='Train '+label\n",
    "                 )\n",
    "    plt.semilogy(history.epoch,\n",
    "                 history.history['val_loss'],\n",
    "                 color=colors[n], \n",
    "                 label='Val '+label,\n",
    "                 linestyle=\"--\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "def plot_metrics(history):\n",
    "    metrics =  ['loss', 'auc', 'precision', 'recall']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(2,2,n+1)\n",
    "        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        if metric == 'loss':\n",
    "            plt.ylim([0, plt.ylim()[1]])\n",
    "        elif metric == 'auc':\n",
    "            plt.ylim([0.5,1])\n",
    "        else:\n",
    "            plt.ylim([0,1])\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "  cm = metrics.confusion_matrix(labels, predictions > p)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n",
    "  \n",
    "\n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, _ = metrics.roc_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives [%]')\n",
    "    plt.ylabel('True positives [%]')\n",
    "    plt.xlim([-0.5,100.5])\n",
    "    plt.ylim([-0.5,100.5])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% LOAD DATASET REMOVING NOISE\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../../output/df_general.csv', sep=',')\n",
    "split = 0.8\n",
    "del dataset['No.']\n",
    "del dataset['Protocol']\n",
    "del dataset['ip_len']\n",
    "del dataset['prebuffering']\n",
    "del dataset['Time']\n",
    "\n",
    "dataset = dataset.fillna(dataset.mean())\n",
    "neg, pos = np.bincount(dataset['stalling_event'])\n",
    "total = neg + pos\n",
    "weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print(dataset.columns)\n",
    "Ratio10(dataset['stalling_event'],'dataset[stalling_event]')\n",
    "histo(dataset,'stalling_event')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% SPLIT TRAIN/TEST\n"
    }
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "'''X_train = [None] * (len(dataset.columns)-1)\n",
    "X_test = [None] * (len(dataset.columns)-1)'''\n",
    "\n",
    "X_train = np.array([None]*12)\n",
    "X_test = np.array([None]*12)\n",
    "y_train = list()\n",
    "y_test = list()\n",
    "\n",
    "grouped = dataset.groupby(['delay','capture']) \n",
    "#print(grouped.mean())\n",
    "\n",
    "for id, data in grouped:\n",
    "    spl = int(data.shape[0] * split)\n",
    "    restspl = int(data.shape[0] * (1-split))\n",
    "    randnum = randint(0,restspl)\n",
    "    \n",
    "    X_train = np.vstack((X_train,data.values[randnum:randnum+spl,:-3]))\n",
    "    X_test = np.vstack((X_test,data.values[0:randnum-1,:-3]))\n",
    "    X_test = np.vstack((X_test,data.values[randnum+spl:,:-3]))\n",
    "    \n",
    "    y_train.extend(data.values[randnum:randnum+spl,-1])\n",
    "    y_test.extend(data.values[0:randnum-1,-1])\n",
    "    y_test.extend(data.values[randnum+spl:,-1])\n",
    "\n",
    "X_train = X_train[1:]\n",
    "X_test = X_test[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mms = preprocessing.MinMaxScaler()\n",
    "X_train = mms.fit_transform(X_train)\n",
    "X_test = mms.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_test = list(map(int,y_test))\n",
    "y_train = list(map(int,y_train))\n",
    "Ratio10(y_test,'y_test')\n",
    "Ratio10(y_train,'y_train')\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train,columns=dataset.columns[:-3])\n",
    "X_test= pd.DataFrame(data=X_test,columns=dataset.columns[:-3])\n",
    "y_train = pd.DataFrame(data=y_train,columns=['y_train'])\n",
    "y_test= pd.DataFrame(data=y_test,columns=['y_test'])\n",
    "del dataset['delay']\n",
    "del dataset['capture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state = rs)\n",
    "X_train_smote,y_train_smote = smote.fit_resample(X_train.values,y_train.values)\n",
    "X_test_smote,y_test_smote = smote.fit_resample(X_test.values,y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "resampled_steps_per_epoch = np.ceil(2.0*neg/BATCH_SIZE)\n",
    "print(resampled_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_timesteps = X_train.shape[0]\n",
    "n_features = X_train.shape[1]\n",
    "n_outputs = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% Resampled\n"
    }
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "resampled_model = Sequential()\n",
    "resampled_model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu',\n",
    "                               input_shape=(n_timesteps, n_features))))\n",
    "resampled_model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu',\n",
    "                               kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))))\n",
    "#model.add(TimeDistributed(Dropout(0.5)))\n",
    "resampled_model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "resampled_model.add(TimeDistributed(Flatten()))\n",
    "resampled_model.add(RepeatVector(n_outputs))\n",
    "resampled_model.add(CuDNNLSTM(32, kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001),\n",
    "                  recurrent_regularizer=l2(0.0001), activity_regularizer=l2(0.0001),\n",
    "                 return_sequences = True))\n",
    "#resampled_model.add(CuDNNLSTM(50, kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001),\n",
    "#                    recurrent_regularizer=l2(0.001), activity_regularizer=l2(0.001)))\n",
    "#resampled_model.add(Dropout(0.5)) #DESCOMENTADO respecto al email\n",
    "resampled_model.add(Dense(32, activation='relu',kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001)))\n",
    "#resampled_model.add(Dropout(0.5))\n",
    "resampled_model.add(Dense(1, activation='sigmoid'))\n",
    "resampled_model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "resampled_model.summary()\n",
    "\n",
    "resampled_model.fit(X_train_smote, y_train_smote, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "\n",
    "initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')\n",
    "resampled_model.save_weights(initial_weights)\n",
    "\n",
    "resampled_model.load_weights(initial_weights)\n",
    "output_layer = resampled_model.layers[-1] \n",
    "output_layer.bias.assign([0])\n",
    "\n",
    "resampled_history = resampled_model.fit(X_train_smote,\n",
    "                                        y_train_smote,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        epochs=EPOCHS,\n",
    "                                        callbacks = [early_stopping],\n",
    "                                        validation_split=0.2\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% USING test_smote\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_metrics(resampled_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_predictions_resampled = resampled_model.predict(X_train_smote, batch_size=BATCH_SIZE)\n",
    "test_predictions_resampled = resampled_model.predict(X_test_smote, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "resampled_results = resampled_model.evaluate(X_test_smote,\n",
    "                                             y_test_smote,\n",
    "                                             batch_size=BATCH_SIZE, \n",
    "                                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for name, value in zip(resampled_model.metrics_names, resampled_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(y_test_smote, test_predictions_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_roc(\"Train Resampled Smote\", y_train_smote, train_predictions_resampled,  color=colors[2])\n",
    "plot_roc(\"Test Resampled Smote\", y_test_smote, test_predictions_resampled,  color=colors[2], linestyle='--')\n",
    "\n",
    "plt.legend(loc='lower right')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

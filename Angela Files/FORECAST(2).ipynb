{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mLRNbufErB_R",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "78eda746-9970-4641-ff92-3c4d679ed7ee"
   },
   "outputs": [],
   "source": [
    "# univariate multi-step encoder-decoder cnn-lstm\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, RepeatVector, TimeDistributed, Dropout, Embedding\n",
    "\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l1_l2, l1, l2\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "dTolzKWIr8HQ",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def split_dataset(data,split=0.8):\n",
    "    Nrow, Ncol = data.shape\n",
    "    splitRows = math.ceil(Nrow * split)\n",
    "    \n",
    "    train,test = data.loc[0:splitRows], data.loc[splitRows+1:Nrow]\n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Mj_ysIYytCtg",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "\tscores = list()\n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\t# calculate mse\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\t# calculate rmse\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\t# store\n",
    "\t\tscores.append(rmse)\n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "8k9by-EotMmS",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "8YNS6i76tO42",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=200):\n",
    "  # flatten data\n",
    "  data = np.array(train)\n",
    "  data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "  X, y = list(), list()\n",
    "  in_start = 0\n",
    "  # step over the entire history one time step at a time\n",
    "  for _ in range(len(data)):\n",
    "    # define the end of the input sequence\n",
    "    in_end = in_start + n_input\n",
    "    out_end = in_end + n_out\n",
    "    # ensure we have enough data for this instance\n",
    "    if out_end <= len(data):\n",
    "      x_input = data[in_start:in_end, -3]\n",
    "      x_input = x_input.reshape((len(x_input), 1))\n",
    "      X.append(x_input)\n",
    "      y.append(data[in_end:out_end, -3])\n",
    "    # move along one time step\n",
    "    in_start += 1\n",
    "  return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "pl5fPuSYtvWD",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def build_model_2(train, n_input):\n",
    "# prepare data\n",
    "  train_x, train_y = to_supervised(train, n_input)\n",
    "  # define parameters\n",
    "  verbose, epochs, batch_size = 0, 50, 200\n",
    "  n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "  # reshape output into [samples, timesteps, features]\n",
    "  train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1)) #Features es 3?\n",
    "\n",
    "  # define model\n",
    "  model = Sequential()\n",
    "  model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu',\n",
    "                                 kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001),\n",
    "                                 input_shape=(None, n_timesteps, n_features))))\n",
    "  model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu',\n",
    "                                 kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))))\n",
    "  model.add(TimeDistributed(Dropout(0.5)))\n",
    "  model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "  model.add(TimeDistributed(Flatten()))\n",
    "  model.add(RepeatVector(n_outputs))\n",
    "  #model.add(CuDNNLSTM(32, kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001),\n",
    "   #                 recurrent_regularizer=l2(0.0001), activity_regularizer=l2(0.0001),\n",
    "    #               return_sequences = False))\n",
    "  model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "  #model.add(CuDNNLSTM(50, kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001),\n",
    "  #                    recurrent_regularizer=l2(0.001), activity_regularizer=l2(0.001)))\n",
    "  #model.add(Dropout(0.5)) #DESCOMENTADO respecto al email\n",
    "  model.add(Dense(32, activation='relu',kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001)))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='nadam')#, metrics=['accuracy'])\n",
    "\n",
    "  # fit network\n",
    "  model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "FtH26Y4RKsbK",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "  # prepare data\n",
    "  train_x, train_y = to_supervised(train, n_input)\n",
    "  #train_x shape(1093, 14, 1) <- inputs\n",
    "  #train_y shape(1093, 7)\n",
    "  # define parameters\n",
    "  verbose, epochs, batch_size = 1, 2, 200\n",
    "  n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "  # reshape output into [samples, timesteps, features]\n",
    "  train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "  # define model\n",
    "  model = Sequential()\n",
    "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  model.add(Flatten())\n",
    "  model.add(RepeatVector(n_outputs))\n",
    "\n",
    "  model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "\n",
    "  model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "  #model.add(TimeDistributed(Dropout(0.2)))\n",
    "  model.add(TimeDistributed(Dense(1)))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='nadam')\n",
    "  # fit network\n",
    "  model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "jbWk-5NUueG7",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "  # make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "  # flatten data\n",
    "  data = np.array(history)\n",
    "  data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "  # retrieve last observations for input data\n",
    "  input_x = data[-n_input:, -3]\n",
    "  # reshape into [1, n_input, 1]\n",
    "  input_x = input_x.reshape((1, len(input_x), 1))\n",
    "  # forecast the next week\n",
    "  yhat = model.predict(input_x, verbose=0)\n",
    "  # we only want the vector forecast\n",
    "  yhat = yhat[0]\n",
    "  print(yhat)\n",
    "  return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "8_fhRxJ-uuNg",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "\t# fit model\n",
    "\tmodel = build_model(train, n_input)\n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "\t# walk-forward validation over each week\n",
    "\tpredictions = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# predict the week\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)\n",
    "\t\t# store the predictions\n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\t# get real observation and add to history for predicting the next week\n",
    "\t\thistory.append(test[i, :])\n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = np.array(predictions)\n",
    "\tscore, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "\treturn score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "JvWtv2NAvJpT",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "df = pd.read_csv('../output/df_general.csv')\n",
    "print(df.columns)\n",
    "\n",
    "df = df.drop(columns=['No','Protocol','tcp_flags_fin','tcp_flags_syn'])\n",
    "df = df.drop(columns=['tcp_flags_push','tcp_flags_cwr'])\n",
    "df = df.drop(columns=['tcp_flags_ecn','tcp_flags_urg','prebuffering','Time'])\n",
    "df = df.astype(dtype= {\"pkt_length\":np.uint16, 'ip_len':np.uint16, 'tcp_window_size_value':np.uint16,\n",
    "                       'stalling_event':np.uint8, 'measure':np.uint8,\n",
    "                       'capture':np.uint8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "pGu5f5fptxfN",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "df.shape #1-232183 0-75 test\n",
    "         #232184-739039 train\n",
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "eQpF6Zc1teD2",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# load the new file\n",
    "#dataset = read_csv('household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n",
    "# split into train and test\n",
    "train, test = split_dataset(df.values)\n",
    "# evaluate model and get scores\n",
    "n_input = 200\n",
    "#score, scores = evaluate_model(train, test, n_input)\n",
    "# summarize scores\n",
    "#summarize_scores('lstm', score, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "kzq13NfF2Kt8",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "2ypC4pF5y3av",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "jeje = np.array(train)\n",
    "jeje.shape #232182, 1, 6 #6 features, 1 ventana, 232182 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "UPIFsprCy55t",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "train_x, train_y = to_supervised(train, n_input=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "RBOiYFKX3ExK",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "train_x, train_y = to_supervised(train, 200, n_out=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "LBFQSYtq3Ol0",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "print(train_x.shape) #200 de input, 100 output\n",
    "print(train_y.shape)\n",
    "\n",
    "print(type(train_x))\n",
    "print(type(train_y))\n",
    "history=[x for x in train]\n",
    "print(type(history))\n",
    "data= np.array(history)\n",
    "data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data\n",
    "input_x = data[-100000:, -3]\n",
    "print(input_x)\n",
    "#print(data.shape)\n",
    "#print(sum(data==0)\n",
    "input_x = input_x.reshape((1, len(input_x), 1))\n",
    "print(input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "RTt51rI7IMYz",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "verbose, epochs, batch_size = 0, 50, 100\n",
    "n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "# reshape output into [samples, timesteps, features]\n",
    "train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1)) #Features es 3?\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu',\n",
    "                               input_shape=(n_timesteps, n_features))))\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu',\n",
    "                               kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))))\n",
    "#model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(RepeatVector(n_outputs))\n",
    "model.add(CuDNNLSTM(32, kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001),\n",
    "                  recurrent_regularizer=l2(0.0001), activity_regularizer=l2(0.0001),\n",
    "                 return_sequences = True))\n",
    "#model.add(CuDNNLSTM(50, kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001),\n",
    "#                    recurrent_regularizer=l2(0.001), activity_regularizer=l2(0.001)))\n",
    "#model.add(Dropout(0.5)) #DESCOMENTADO respecto al email\n",
    "model.add(Dense(32, activation='relu',kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001)))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "model.fit(train_x, train_y, epochs=20, batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Hj2De1BLJN_E",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377.0
    },
    "outputId": "be69aaac-7d9c-4c06-ee79-d43a37ff5c87"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0831 16:00:54.685841 140382719793024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0831 16:00:54.725475 140382719793024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0831 16:00:54.732731 140382719793024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0831 16:00:54.804379 140382719793024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0831 16:00:55.177259 140382719793024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0831 16:00:55.204165 140382719793024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0831 16:00:55.211299 140382719793024 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0831 16:00:56.334639 140382719793024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 49000/231783 [=====>........................] - ETA: 29:06 - loss: 0.7164"
     ]
    }
   ],
   "source": [
    "build_model(train, n_input)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "FORECAST.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}